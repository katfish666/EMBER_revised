{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "import distance\n",
    "import math\n",
    "import pandas as pd\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "import os\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "def allDone():\n",
    "    urL = 'https://www.wavsource.com/snds_2020-10-01_3728627494378403/animals/cat_meow2.wav'\n",
    "    display(Audio(url=urL, autoplay=True))\n",
    "# allDone()\n",
    "\n",
    "seedy = 666\n",
    "random.seed(seedy)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5766 641\n"
     ]
    }
   ],
   "source": [
    "train_motifs = np.genfromtxt('data_dev/train_motifs.csv',dtype='U')\n",
    "train_motifxFamMatrix = np.genfromtxt('data_dev/train_motifxFamMatrix.csv',delimiter=',',dtype=int)\n",
    "test_motifs = np.genfromtxt('data_dev/test_motifs.csv',dtype='U')\n",
    "test_motifxFamMatrix = np.genfromtxt('data_dev/test_motifxFamMatrix.csv',delimiter=',',dtype=int)\n",
    "\n",
    "fams = np.genfromtxt('data_dev/fams.csv',dtype='U')\n",
    "\n",
    "all_motifs = np.hstack([train_motifs,test_motifs])\n",
    "all_motifxFamMatrix = np.vstack([train_motifxFamMatrix,test_motifxFamMatrix])\n",
    "\n",
    "X_train, X_val = train_test_split(range(len(train_motifs)), test_size=0.1, random_state=666)\n",
    "\n",
    "print(len(X_train), len(X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "AMINOS = 'XWGSAELQDMPFTRIHVNCY_K'\n",
    "\n",
    "def get_oneHot_motifs(motifs, AMINOS=AMINOS):\n",
    "    oneHot_motifs = []\n",
    "    for motif in motifs:\n",
    "        one_hotted = np.zeros((len(motif), len(AMINOS)),dtype=float)\n",
    "        for i,aa in enumerate(motif):\n",
    "            hot = AMINOS.find(aa)\n",
    "            one_hotted[i][hot] = 1\n",
    "        oneHot_motifs.append(one_hotted)\n",
    "    oneHot_motifs = np.asarray(oneHot_motifs)\n",
    "    oneHot_motifs = np.swapaxes(oneHot_motifs,1,2)\n",
    "    return oneHot_motifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # # # # #\n",
    "my_embed = '14' \n",
    "# # # # # # # #\n",
    "\n",
    "#############################################\n",
    "# Get Siamese embedding coords.\n",
    "#############################################\n",
    "\n",
    "embedding = np.genfromtxt('MODELS_siam/emb_%s_embedding.csv' % my_embed,delimiter=',',dtype=float)\n",
    "\n",
    "train_embedding = embedding[ :len(train_motifs) ]\n",
    "test_embedding = embedding[ len(train_motifs): ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[35, 97, 103, 110, 44, 122, 81, 119]\n",
      "['PKA' 'AKT' 'CDK' 'MAPK' 'SRC' 'CK2' 'PKC' 'PIKK']\n",
      "[[0.55248832 0.25593144 0.08403827 0.08104859 0.         0.0536795\n",
      "  0.20530341 0.03245439]\n",
      " [0.25593144 0.8164331  0.10530056 0.09521106 0.03300859 0.10081339\n",
      "  0.34600292 0.06116343]\n",
      " [0.08403827 0.10530056 0.40598022 0.24504389 0.09240628 0.18606964\n",
      "  0.10720635 0.09568013]\n",
      " [0.08104859 0.09521106 0.24504389 0.44745779 0.09185111 0.1974867\n",
      "  0.09646782 0.09364453]\n",
      " [0.         0.03300859 0.09240628 0.09185111 0.47553156 0.03655681\n",
      "  0.03802115 0.03964883]\n",
      " [0.0536795  0.10081339 0.18606964 0.1974867  0.03655681 1.\n",
      "  0.09344097 0.08501536]\n",
      " [0.20530341 0.34600292 0.10720635 0.09646782 0.03802115 0.09344097\n",
      "  0.53143529 0.06488062]\n",
      " [0.03245439 0.06116343 0.09568013 0.09364453 0.03964883 0.08501536\n",
      "  0.06488062 0.41866353]]\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "# Get fam distance matrix for Phylo MSE loss.\n",
    "#############################################\n",
    "\n",
    "\n",
    "\n",
    "all_fams = (np.genfromtxt('data_dev/fam_distances_blos62/fams.csv',dtype='U'))\n",
    "dist_matrix = (np.genfromtxt('data_dev/fam_distances_blos62/dist_matrix.csv',delimiter=',',dtype=float))\n",
    "\n",
    "fam_idc = [np.where(all_fams==fam)[0][0] for fam in fams]\n",
    "print(fam_idc)\n",
    "\n",
    "fam_dist_matrix = dist_matrix[fam_idc][:,fam_idc]\n",
    "# print(fam_dist_matrix)\n",
    "        \n",
    "        \n",
    "print(fams)\n",
    "\n",
    "# normalize fam distances\n",
    "fMax = np.max(fam_dist_matrix)\n",
    "fMin = np.min(fam_dist_matrix)\n",
    "\n",
    "fam_dist_matrix_scaled = np.array((fam_dist_matrix))\n",
    "for i in range(len(fams)):\n",
    "    for j in range(len(fams)):\n",
    "        fam_dist_matrix_scaled[i][j] = float(fam_dist_matrix[i][j]-fMin)/(fMax-fMin) \n",
    "fam_dist_matrix = fam_dist_matrix_scaled\n",
    "\n",
    "print(fam_dist_matrix_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Get normalized fam counts for class-weighted loss. (Because of imbalanced classes.)\n",
    "#############################################\n",
    "\n",
    "fam_counts = []\n",
    "for fIdx,fam in enumerate(train_motifxFamMatrix.T):\n",
    "    fam_counts.append(np.sum(train_motifxFamMatrix.T[fIdx]))\n",
    "fam_counts = np.array(fam_counts)\n",
    "\n",
    "normalized_fam_counts = fam_counts / np.sqrt(np.sum(fam_counts**2))\n",
    "for i in range(len(normalized_fam_counts)):\n",
    "    normalized_fam_counts[i] = 1-normalized_fam_counts[i]\n",
    "normalized_fam_counts = torch.from_numpy( np.array(normalized_fam_counts)) \n",
    "normalized_fam_counts = normalized_fam_counts.double().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stacked_features(motifs, embeddings):\n",
    "    oneHot_motifs = get_oneHot_motifs(motifs)\n",
    "    squished_oneHots = oneHot_motifs.reshape(oneHot_motifs.shape[0],oneHot_motifs.shape[1]*\n",
    "                                         oneHot_motifs.shape[2])\n",
    "    stacked_features = np.hstack((squished_oneHots,embeddings))\n",
    "    stacked_features = torch.tensor(stacked_features)\n",
    "    return stacked_features        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"motifs:\",get_oneHot_motifs(test_motifs).shape,22*15)\n",
    "# print(\"embeds:\",test_embedding.shape)\n",
    "# print(\"stacked:\",get_stacked_features(test_motifs,test_embedding).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cannot used stratified K fold because I have multi-label data ... Will try a thing from stackoverflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seedy)\n",
    "\n",
    "## https://stats.stackexchange.com/questions/65828/\n",
    "## how-to-use-scikit-learns-cross-validation-functions-on-multi-label-classifiers\n",
    "\n",
    "def proba_mass_split(y, folds=5):\n",
    "    obs, classes = y.shape\n",
    "    dist = y.sum(axis=0).astype('float')\n",
    "    dist /= dist.sum()\n",
    "    index_list = []\n",
    "    fold_dist = np.zeros((folds, classes), dtype='float')\n",
    "    for _ in range(folds):\n",
    "        index_list.append([])\n",
    "    for i in range(obs):\n",
    "        if i < folds:\n",
    "            target_fold = i\n",
    "        else:\n",
    "            normed_folds = fold_dist.T / fold_dist.sum(axis=1)\n",
    "            how_off = normed_folds.T - dist\n",
    "            target_fold = np.argmin(np.dot((y[i] - .5).reshape(1, -1), how_off.T))\n",
    "        fold_dist[target_fold] += y[i]\n",
    "        index_list[target_fold].append(i)\n",
    "#     print(\"Fold distributions are\")\n",
    "#     print(fold_dist)\n",
    "    return index_list"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for split in splits:\n",
    "    for epoch in epochs:\n",
    "        train(X_train)\n",
    "        evaluate(X_val)\n",
    "    evaluate(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = proba_mass_split(train_motifxFamMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "# from scipy import interp\n",
    "\n",
    "### 'macro' means each class is weighted evenly. ###\n",
    "\n",
    "def get_microROC(y_test, y_score):\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    \n",
    "    for i in range(len(fams)):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    fpr_micro, tpr_micro, _ = roc_curve(y_test.ravel(), y_score.ravel())\n",
    "    return auc(fpr_micro, tpr_micro)\n",
    "\n",
    "def get_macroAP(y_test, y_score):\n",
    "    precision = dict()\n",
    "    recall = dict()\n",
    "    average_precision = dict()\n",
    "    for i in range(len(fams)):\n",
    "        precision[i], recall[i], _ = precision_recall_curve(y_test[:, i], y_score[:, i])\n",
    "        average_precision[i] = average_precision_score(y_test[:, i], y_score[:, i])\n",
    "    return average_precision_score(y_test, y_score, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_loader, val_loader, model, optimizer, batch_size, num_epochs, stopper='loss', \n",
    "                version='seq-coord', this_loss='phylo',mat_kind='0',operator='BCE',power=1.0):\n",
    "    \n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "    \n",
    "    best_val = float('inf')\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    \n",
    "    best_model = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch\",epoch+1)\n",
    "        \n",
    "        for phase in ['train','validate']:\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_acc = 0.0\n",
    "\n",
    "            if phase=='train':\n",
    "                loader = train_loader\n",
    "                model.train()\n",
    "            else:\n",
    "                loader = val_loader\n",
    "                model.eval()\n",
    "\n",
    "            for inputs,labels in loader:\n",
    "\n",
    "                inputs = inputs.float().to(device)\n",
    "                labels = labels.float().to(device)\n",
    "\n",
    "                motif = inputs[:,:-100].reshape( batch_size, len(AMINOS), len(train_motifs[0]) )\n",
    "                coords = inputs[:,-100:]\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase=='train'): ### only enable gradient if in \"train\" phase\n",
    "\n",
    "                    outputs = model.forward(motif, coords, version)\n",
    "\n",
    "                    if this_loss=='phylo' and version=='seq-coord':\n",
    "                        loss = phylo_error(outputs,labels,mat_kind,operator,power) # # #   # # #  # # #  # # #  # # #  # # #  # # #  # # #  # # #  # # #  # # #  # # #  # # # \n",
    "                    \n",
    "                    elif this_loss!='phylo':\n",
    "                        criterion = nn.BCELoss() # BCE loss is the usual loss to use.\n",
    "                        loss = criterion(outputs, labels)\n",
    "                        \n",
    "#                     loss = loss * normalized_fam_counts.float()\n",
    "#                     loss = loss.mean()\n",
    "\n",
    "                    if phase=='train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                ### get performance accuracy\n",
    "                y_test = np.asarray(labels.cpu())\n",
    "                y_score = outputs.cpu().detach().numpy()\n",
    "                acc = get_microROC(y_test, y_score)\n",
    "\n",
    "                running_acc += acc\n",
    "            \n",
    "            loss = running_loss / len(loader) \n",
    "            acc =  running_acc / len(loader) \n",
    "            \n",
    "            if phase=='train':\n",
    "                train_losses.append(loss)\n",
    "                train_accs.append(acc)\n",
    "                \n",
    "            elif phase=='validate':\n",
    "                val_losses.append(loss)\n",
    "                val_accs.append(acc)\n",
    "                \n",
    "                if stopper=='loss':\n",
    "                    best_loss = loss\n",
    "                    best_model = copy.deepcopy(model.state_dict())\n",
    "                elif stopper=='acc':\n",
    "                    best_acc = acc\n",
    "                    best_model = copy.deepcopy(model.state_dict())\n",
    "                    \n",
    "                \n",
    "            print(\"~ %s LOSS: %5.3f | ACC: %5.3f\" % (phase,loss,acc))\n",
    "        \n",
    "    return (best_model, train_losses, train_accs, val_losses, val_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(motifs,embedding,motifxFamMatrix,idc,my_batch):\n",
    "\n",
    "    these_motifs = motifs[idc]\n",
    "    this_embedding = embedding[idc]\n",
    "    \n",
    "    X = get_stacked_features(these_motifs,this_embedding)\n",
    "    Y = torch.tensor(motifxFamMatrix[idc])\n",
    "    dataset = data_utils.TensorDataset(X, Y)\n",
    "    loader = data_utils.DataLoader(dataset, batch_size=my_batch, shuffle=True, drop_last=True)\n",
    "    \n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Model(nn.Module):\n",
    "    \n",
    "### Conv1d(in_channels, out_channels, kernel_size, stride)\n",
    "### (vocab_dims, out_dims, kernel_size_spread_motif)\n",
    "        \n",
    "    def __init__(self,conv_drpt=0.0,mlp_drpt=0.0):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        #### MOTIF NET ####\n",
    "        self.conv1 = nn.Conv1d(22, 16, kernel_size=5, padding=2)\n",
    "        self.conv2 = nn.Conv1d(16, 32, kernel_size=5, padding=2) \n",
    "        self.conv3 = nn.Conv1d(32, 64, kernel_size=5, padding=2)\n",
    "        \n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        self.penult = nn.Linear(64, 32) \n",
    "        \n",
    "        #### COORD NET ####\n",
    "        self.mlp1 = nn.Linear(100, 112) \n",
    "        self.bn1 = nn.BatchNorm1d(112)\n",
    "        self.mlp2 = nn.Linear(112, 96)\n",
    "        self.bn2 = nn.BatchNorm1d(96)\n",
    "        self.mlp3 = nn.Linear(96, 64)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.penult = nn.Linear(64, 32) \n",
    "        \n",
    "        ### CAT LAYERS ###\n",
    "        self.penult = nn.Linear(64, 32) \n",
    "        self.out = nn.Linear(32, len(fams))\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        #### MISC LAYERS ####\n",
    "        \n",
    "        self.relu = nn.LeakyReLU()\n",
    "        self.conv_drpt = nn.Dropout(p = conv_drpt)\n",
    "        self.mlp_drpt = nn.Dropout(p = mlp_drpt)\n",
    "        self.ablate = nn.Dropout(p = 1.0)\n",
    "        \n",
    "    def forward(self, oneHot_motif, coords, version='seq-coord'): \n",
    "#         print(oneHot_motif.shape)\n",
    "        \n",
    "        #### MOTIF NET ####\n",
    "        conv1 = self.conv1(oneHot_motif.float())\n",
    "        conv1 = self.relu(conv1)\n",
    "        conv1 = self.pool(conv1)\n",
    "#         print(\"Conv 1:\",conv1.shape)\n",
    "        conv2 = self.conv2(conv1)\n",
    "        conv2 = self.relu(conv2)\n",
    "        conv2 = self.pool(conv2)\n",
    "#         print(\"Conv 2:\",conv2.shape)\n",
    "        conv3 = self.conv3(conv2)\n",
    "        conv3 = self.relu(conv3)\n",
    "        conv3 = self.pool(conv3)\n",
    "#         print(\"Conv 3:\",conv3.shape)\n",
    "        seq_out = conv3.view(conv3.size()[0], -1)\n",
    "        seq_out = self.penult(seq_out) ## SEQ PENULT\n",
    "        seq_out = self.relu(seq_out)\n",
    "        seq_out = self.conv_drpt(seq_out)\n",
    "#         print(\"Conv out:\",seq_out.shape)\n",
    "        \n",
    "        #### COORD NET ####\n",
    "        mlp1 = self.mlp1(coords)\n",
    "\n",
    "        mlp1 = self.relu(mlp1)\n",
    "#         mlp1 = self.bn1(mlp1)\n",
    "        mlp1 = self.mlp_drpt(mlp1)\n",
    "#         print(\"MLP 1:\",mlp1.shape)\n",
    "        mlp2 = self.mlp2(mlp1)\n",
    "        \n",
    "        mlp2 = self.relu(mlp2)\n",
    "#         mlp2 = self.bn2(mlp2)\n",
    "        mlp2 = self.mlp_drpt(mlp2)\n",
    "#         print(\"MLP 2:\",mlp2.shape)\n",
    "        mlp3 = self.mlp3(mlp2)\n",
    "        \n",
    "        mlp3 = self.relu(mlp3)\n",
    "#         mlp3 = self.bn3(mlp3)\n",
    "        mlp3 = self.mlp_drpt(mlp3)\n",
    "#         print(\"MLP 3:\",mlp3.shape)\n",
    "        coord_out = self.penult(mlp3)\n",
    "#         coord_out = self.bn4(coord_out)\n",
    "        coord_out = self.relu(coord_out)\n",
    "#         print(\"MLP Penult:\",coord_out.shape)\n",
    "        \n",
    "        if version=='seq-coord':\n",
    "            coords_out = self.mlp_drpt(coord_out) # seqCoord version\n",
    "        else: \n",
    "            coords_out = self.ablate(coord_out) # seq-only version\n",
    "        \n",
    "        cat = torch.cat((seq_out,coords_out), 1)\n",
    "        cat = self.penult(cat)        \n",
    "        out = self.out(cat)\n",
    "        out = self.sigmoid(out)\n",
    "#         print(out.shape)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phylo_error(output, target, mat_kind='0', operator='BCE',power=1.0):\n",
    "        \n",
    "    weights = np.ones((output.shape[0],output.shape[1]))\n",
    "\n",
    "    for i,t in enumerate(target):\n",
    "        t = t.cpu()\n",
    "        wIdc = np.where(t.detach().numpy()==1)[0]\n",
    "\n",
    "        if len(wIdc)==0:\n",
    "            weights[i] = 0.000001\n",
    "            continue\n",
    "        theseWeights = np.ones((len(fams)))\n",
    "        \n",
    "        # want inter-fam weights to be distance (larger number, less related = more weight) ?\n",
    "        # want intra-fam weights to be similarity (larger number = more weight) ?\n",
    "        \n",
    "        for wIdx in wIdc:\n",
    "            if mat_kind=='0':\n",
    "                thisWeight = 1.00 - famDistMatrix[wIdx].copy() # inter-fam\n",
    "                thisWeight[wIdx] =  famDistMatrix[wIdx][wIdx].copy() # intra-fam\n",
    "            elif mat_kind=='1':\n",
    "                thisWeight = famDistMatrix[wIdx].copy() # inter-fam\n",
    "                thisWeight[wIdx] =  1.00 - famDistMatrix[wIdx][wIdx].copy() # intra-fam\n",
    "\n",
    "            theseWeights+=thisWeight # add to existing list of fam distances, respectively (element wise)\n",
    "            \n",
    "        fWeight = theseWeights/len(fams) # take median / average\n",
    "        weights[i] = fWeight \n",
    "    weights = torch.tensor(weights)\n",
    "    weights = weights.to(device)\n",
    "    \n",
    "    if operator=='MSE':\n",
    "        answer = (output.double()-target.double()+1e-6) * (weights.double()).mean() \n",
    "    elif operator=='BCE':\n",
    "        crit = nn.BCELoss()\n",
    "        answer = crit(output, target) * weights.mean().float()\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s = time.time()\n",
    "\n",
    "my_version = 'seq-coord'         # seq-coord ... seq\n",
    "my_loss = 'phylo'            # phylo ... canon\n",
    "\n",
    "# ONLY IF DOING PHYLO LOSS\n",
    "operator='MSE'\n",
    "mat_kind='0'\n",
    "power=1.0\n",
    "\n",
    "my_conv_drpt = 0.0\n",
    "my_mlp_drpt = 0.0\n",
    "\n",
    "my_stopper = 'loss'\n",
    "my_batch = 32\n",
    "my_epochs = 15\n",
    "my_lr = 0.0015\n",
    "\n",
    "all_train_losses = []\n",
    "all_train_accs = []\n",
    "all_val_losses = []\n",
    "all_val_accs = []\n",
    "\n",
    "all_models = []\n",
    "\n",
    "for i,fold in enumerate(folds):\n",
    "    \n",
    "    print(\"\\n* * * * * * * * FOLD %d * * * * * * * *\\n\" %(i+1))\n",
    "    \n",
    "    fold_val_idc = fold\n",
    "    fold_train_idc = [x for x in range(len(train_motifs)) if x not in fold_val_idc]\n",
    "    \n",
    "    train_loader = get_loader(train_motifs,train_embedding,train_motifxFamMatrix,fold_train_idc,my_batch)\n",
    "    val_loader = get_loader(train_motifs,train_embedding,train_motifxFamMatrix,fold_val_idc,my_batch)\n",
    "    \n",
    "    model = Model(my_conv_drpt,my_mlp_drpt)\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(),lr = my_lr)\n",
    "    \n",
    "    (best_model, train_losses, train_accs, val_losses, val_accs) = train_model(train_loader, val_loader,\n",
    "                                                                               model, optimizer,my_batch, \n",
    "                                                                               my_epochs,my_stopper,my_version,\n",
    "                                                                               my_loss, mat_kind,operator,power)\n",
    "    \n",
    "    all_models.append(best_model)\n",
    "    all_train_losses.append(train_losses)\n",
    "    all_train_accs.append(train_accs)\n",
    "    all_val_losses.append(val_losses)\n",
    "    all_val_accs.append(val_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n TIME: %5.3f mins\" % ((time.time()-s)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = '16'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.mkdir(\"MODELS_multiclass/%s/\" % run)\n",
    "for i,model_weights in enumerate(all_models):\n",
    "#     print(model_weights)\n",
    "    torch.save(model_weights, \"MODELS_multiclass/%s/%d_weights\" % (run,i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_losses_arr = np.zeros(my_epochs)\n",
    "all_train_accs_arr = np.zeros(my_epochs)\n",
    "all_val_losses_arr = np.zeros(my_epochs)\n",
    "all_val_accs_arr = np.zeros(my_epochs)\n",
    "\n",
    "for i in range(my_epochs):\n",
    "    all_train_losses_arr[i] = sum([all_train_losses[j][i] for j in range(len(folds))]) / len(folds)\n",
    "    all_train_accs_arr[i] = sum([all_train_accs[j][i] for j in range(len(folds))]) / len(folds)\n",
    "    all_val_losses_arr[i] = sum([all_val_losses[j][i] for j in range(len(folds))]) / len(folds)\n",
    "    all_val_accs_arr[i] = sum([all_val_accs[j][i] for j in range(len(folds))]) / len(folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(7.5,5))\n",
    "plt.plot(all_train_losses_arr,label='Train loss',c='blue')\n",
    "plt.plot(all_train_accs_arr,label='Train acc',c='red')\n",
    "plt.plot(all_val_losses_arr,label='Val loss',c='green')\n",
    "plt.plot(all_val_accs_arr,label='Val acc',c='orange')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend(loc='center right')\n",
    "plt.savefig(\"FIGS_multiclass/\" + run + \"_loss-acc\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, thresh=0.5):\n",
    "    \n",
    "    model.eval()\n",
    "    loader = get_loader(test_motifs,test_embedding,test_motifxFamMatrix,range(len(test_motifs)),len(test_motifs))\n",
    "    \n",
    "    for inputs, labels in loader:\n",
    "        \n",
    "        inputs = inputs.float().to(device)\n",
    "        labels = labels.float().to(device)\n",
    "\n",
    "        motif = inputs[:,:-100].reshape( len(test_motifs), len(AMINOS), len(train_motifs[0]) )\n",
    "        coords = inputs[:,-100:]\n",
    "        \n",
    "        outputs = model.forward(motif, coords, my_version)\n",
    "        \n",
    "        accuracy = 0\n",
    "        totTrues = 0\n",
    "        for i,out in enumerate(outputs):\n",
    "            pred = np.where(out.cpu().detach().numpy() > thresh)[0]\n",
    "            true = np.where(labels.data.cpu()[i]==1)[0]\n",
    "            totTrues += len(true)\n",
    "            accuracy += len(pred)\n",
    "\n",
    "        y_score = outputs.cpu().detach().numpy()\n",
    "        y_test = np.asarray(labels.cpu())\n",
    "                \n",
    "        return y_test, y_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run = '??'\n",
    "\n",
    "# all_y_scores = []\n",
    "# all_y_test = []\n",
    "# for i in range(5):\n",
    "    \n",
    "#     model = Model()\n",
    "#     model = model.to(device)\n",
    "#     model.load_state_dict(torch.load(\"MODELS_multiclass/%s/%d_weights\" % (run,i)))\n",
    "    \n",
    "#     y_test, y_score = eval_model(model)\n",
    "#     all_y_scores.extend(y_score)\n",
    "#     all_y_test.extend(y_test)\n",
    "    \n",
    "# y_score = np.asarray(all_y_scores)\n",
    "# y_test = np.asarray(all_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_y_scores = []\n",
    "all_y_test = []\n",
    "for model_weights in all_models:\n",
    "    \n",
    "    model = Model()\n",
    "    model = model.to(device)\n",
    "    model.load_state_dict(model_weights)\n",
    "    \n",
    "    y_test, y_score = eval_model(model)\n",
    "    all_y_scores.extend(y_score)\n",
    "    all_y_test.extend(y_test)\n",
    "    \n",
    "y_score = np.asarray(all_y_scores)\n",
    "y_test = np.asarray(all_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ROC\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy import interp\n",
    "import numpy as np\n",
    "\n",
    "FAM_IDC = [x for x in range(len(fams))]\n",
    "\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in FAM_IDC:\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "# Compute macro-average ROC curve and ROC area\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in FAM_IDC]))\n",
    "\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in FAM_IDC:\n",
    "    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= len(FAM_IDC)\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PRECISION-RECALL\n",
    "\n",
    "from itertools import chain\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "average_precision = dict()\n",
    "\n",
    "for i in range(len(fams)):\n",
    "    precision[i], recall[i], _ = precision_recall_curve(y_test[:, i], y_score[:, i])\n",
    "    average_precision[i] = average_precision_score(y_test[:, i], y_score[:, i])\n",
    "\n",
    "precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(y_test.ravel(), y_score.ravel())\n",
    "\n",
    "average_precision[\"micro\"] = average_precision_score(y_test, y_score, average=\"micro\")\n",
    "average_precision[\"macro\"] = average_precision_score(y_test, y_score, average=\"macro\")\n",
    "\n",
    "print('AUROC macro: {0:0.3f}'.format(roc_auc[\"macro\"]))\n",
    "print('AUROC micro: {0:0.3f}'.format(roc_auc[\"micro\"]))\n",
    "print('\\nAP macro: {0:0.3f}'.format(average_precision[\"macro\"]))\n",
    "print('AP micro: {0:0.3f}'.format(average_precision[\"micro\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# from classifierNet import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('default')\n",
    "fig, (ax1, ax2) = plt.subplots(2,figsize=(5,11))\n",
    "lw = 1.25\n",
    "\n",
    "############################################################\n",
    "###################  AUROC #################################\n",
    "############################################################\n",
    "\n",
    "colors = cycle(['red','magenta','orange', 'gold','yellowgreen','turquoise', 'blue', 'purple']) \n",
    "lines = []\n",
    "labels = []\n",
    "\n",
    "l, = ax1.plot(0,0,color='white')\n",
    "lines.append(l)\n",
    "labels.append('micro-average (area = {0:0.3f})'.format(roc_auc[\"micro\"]))\n",
    "l, = ax1.plot(0,0,color='white')\n",
    "lines.append(l)\n",
    "labels.append('macro-average (area = {0:0.3f})'.format(roc_auc[\"macro\"]))\n",
    "\n",
    "for i, color in zip( range( len(fams) ), colors):\n",
    "    fclass = fams[i]\n",
    "    if fclass=='AKT':\n",
    "        fclass = 'Akt'\n",
    "    if fclass=='SRC':\n",
    "        fclass = 'Src'    \n",
    "    l, = ax1.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "             label='{0} (area = {1:0.3f})'.format(fclass, roc_auc[i]))\n",
    "    labels.append('{0} (area = {1:0.3f})'.format(fclass, roc_auc[i]))\n",
    "    lines.append(l)\n",
    "\n",
    "fig.subplots_adjust(hspace=.275)\n",
    "ax1.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "ax1.set_xlim([0.0, 1.0])\n",
    "ax1.set_ylim([0.0, 1.05])\n",
    "ax1.tick_params(axis=\"x\", labelsize=10)\n",
    "ax1.tick_params(axis=\"y\", labelsize=10)\n",
    "ax1.set_xlabel('False Positive Rate',fontsize=12)\n",
    "ax1.set_ylabel('True Positive Rate',fontsize=12) \n",
    "ax1.set_title('ROC curve per kinase family',fontsize=12) \n",
    "labels, lines = zip(*sorted(zip(labels, lines), key=lambda t: t[0], reverse=False))\n",
    "ax1.legend(lines, labels, loc=(0.475, .0175), fontsize=7.5)  # (1.05, .0)\n",
    "\n",
    "############################################################\n",
    "###############  PRECISION-RECALL ##########################\n",
    "############################################################\n",
    "\n",
    "colors = cycle(['red','magenta','orange', 'gold','yellowgreen','turquoise', 'blue', 'purple']) \n",
    "lines = []\n",
    "labels = []\n",
    "\n",
    "l, = ax2.plot(0,0,color='white')\n",
    "lines.append(l)\n",
    "labels.append('micro-average (area = {0:0.3f})'.format(average_precision[\"micro\"]))\n",
    "l, = ax2.plot(0,0,color='white')\n",
    "lines.append(l)\n",
    "labels.append('macro-average (area = {0:0.3f})'.format(average_precision[\"macro\"]))\n",
    "\n",
    "for i, color in zip(range( len(fams) ), colors):\n",
    "    l, = ax2.plot(recall[i], precision[i], color=color, lw=lw)\n",
    "    lines.append(l)\n",
    "    fclass = fams[i]\n",
    "    if fclass=='AKT':\n",
    "        fclass = 'Akt'\n",
    "    if fclass=='SRC':\n",
    "        fclass = 'Src'\n",
    "    labels.append('{0} (area = {1:0.3f})'.format(fclass, average_precision[i]))\n",
    "\n",
    "ax2.set_xlim([0.0, 1.0])\n",
    "ax2.set_ylim([0.0, 1.05])\n",
    "ax2.tick_params(axis=\"x\", labelsize=10)\n",
    "ax2.tick_params(axis=\"y\", labelsize=10)\n",
    "ax2.set_xlabel('Recall',fontsize=12) \n",
    "ax2.set_ylabel('Precision',fontsize=12) \n",
    "ax2.set_title('Precision-recall curve per kinase family',fontsize=12) \n",
    "\n",
    "labels, lines = zip(*sorted(zip(labels, lines), key=lambda t: t[0], reverse=False))\n",
    "ax2.legend(lines, labels, loc=(0.025, .0175), fontsize=7.5) # (1.05, .0)\n",
    "\n",
    "plt.savefig(\"FIGS_multiclass/%s_roc-prc\" % run, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('AUROC macro: {0:0.3f}'.format(roc_auc[\"macro\"]))\n",
    "print('AUROC micro: {0:0.3f}'.format(roc_auc[\"micro\"]))\n",
    "\n",
    "print('\\nAP macro: {0:0.3f}'.format(average_precision[\"macro\"]))\n",
    "print('AP micro: {0:0.3f}'.format(average_precision[\"micro\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allDone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from itertools import cycle\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "# colors = cycle(['red','magenta','orange', 'gold','yellowgreen',\n",
    "#                 'turquoise', 'blue', 'purple']) \n",
    "\n",
    "# plt.style.use('default')\n",
    "# fig, (ax1, ax2) = plt.subplots(2,figsize=(5,11))\n",
    "# lw = 1.25\n",
    "\n",
    "# ax1.plot(fpr[\"micro\"], tpr[\"micro\"],label='micro-average (area = {0:0.3f})'''.format(roc_auc[\"micro\"]), \n",
    "#          color='white', linestyle=':', linewidth=lw)\n",
    "# ax1.plot(fpr[\"macro\"], tpr[\"macro\"], label='macro-average (area = {0:0.3f})'''.format(roc_auc[\"macro\"]),\n",
    "#          color='white', linestyle=':', linewidth=lw)\n",
    "\n",
    "# for i, color in zip( range( len(fams) ), colors):\n",
    "#     fam = fams[i]\n",
    "#     ax1.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "#              label='{0} (area = {1:0.3f})'\n",
    "#              ''.format(fam, roc_auc[i]))\n",
    "\n",
    "# fig.subplots_adjust(hspace=.275)\n",
    "# ax1.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "# ax1.set_xlim([0.0, 1.0])\n",
    "# ax1.set_ylim([0.0, 1.05])\n",
    "# ax1.tick_params(axis=\"x\", labelsize=10)\n",
    "# ax1.tick_params(axis=\"y\", labelsize=10)\n",
    "# ax1.set_xlabel('False Positive Rate',fontsize=12) #20)\n",
    "# ax1.set_ylabel('True Positive Rate',fontsize=12) #20)\n",
    "# ax1.set_title('ROC curve per kinase family',fontsize=12) #=20)\n",
    "# ax1.legend(loc=(1.05, .0), fontsize=7.5) #12.5)\n",
    "\n",
    "# ########\n",
    "# ########\n",
    "\n",
    "\n",
    "# colors = cycle(['red','magenta','orange', 'gold','yellowgreen',\n",
    "#                 'turquoise', 'blue', 'purple']) \n",
    "\n",
    "# lines = []\n",
    "# labels = []\n",
    "\n",
    "\n",
    "# l, = ax2.plot(recall[\"micro\"], precision[\"micro\"], color='white', lw=1.5,linestyle=':')\n",
    "# lines.append(l)\n",
    "# labels.append('micro-average (area = {0:0.3f})'\n",
    "#               ''.format(average_precision[\"micro\"]))\n",
    "# l, = ax2.plot(recall[\"micro\"], precision[\"micro\"], color='white', lw=1.5,linestyle=':')\n",
    "# lines.append(l)\n",
    "# labels.append('macro-average (area = {0:0.3f})'\n",
    "#               ''.format(average_precision[\"macro\"]))\n",
    "\n",
    "# for i, color in zip(range( len(fams) ), colors):\n",
    "#     l, = plt.plot(recall[i], precision[i], color=color, lw=lw)\n",
    "#     lines.append(l)\n",
    "#     fclass = fams[i]\n",
    "#     labels.append('{0} (area = {1:0.3f})'\n",
    "#                   ''.format(fclass, average_precision[i]))\n",
    "\n",
    "# ax2.set_xlim([0.0, 1.0])\n",
    "# ax2.set_ylim([0.0, 1.05])\n",
    "# ax2.tick_params(axis=\"x\", labelsize=10)\n",
    "# ax2.tick_params(axis=\"y\", labelsize=10)\n",
    "# ax2.set_xlabel('Recall',fontsize=12) #20)\n",
    "# ax2.set_ylabel('Precision',fontsize=12) #20)\n",
    "# ax2.set_title('Precision-recall curve per kinase family',fontsize=12) #=20)\n",
    "# ax2.legend(lines, labels, loc=(1.05, .0), fontsize=7.5) #12.5)\n",
    "\n",
    "# plt.savefig(\"FIGS_multiclass/%s_roc-prc\" % run, bbox_inches='tight')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def MSE_phylo(output, target, kind='00'):\n",
    "        \n",
    "#     weights = np.ones((output.shape[0],output.shape[1]))\n",
    "\n",
    "#     for i,t in enumerate(target):\n",
    "#         t = t.cpu()\n",
    "#         wIdc = np.where(t.detach().numpy()==1)[0]\n",
    "\n",
    "#         if len(wIdc)==0:\n",
    "#             weights[i] = 0.000001\n",
    "#             continue\n",
    "#         theseWeights = np.ones((len(fams)))\n",
    "#         for wIdx in wIdc:\n",
    "            \n",
    "#             if kind=='00':\n",
    "            \n",
    "#             # want inter-fam weights to be distance (larger number, less related = more weight)\n",
    "#             thisWeight = 1.00 - famDistMatrix[wIdx].copy() # list of fam distances from true fam\n",
    "#             # want intra-fam weights to be similarity (larger number = more weight)\n",
    "#             thisWeight[wIdx] =  1.00\n",
    "#             theseWeights+=thisWeight # add to existing list of fam distances, respectively (element wise)\n",
    "            \n",
    "#         fWeight = theseWeights/len(fams) # take median / average\n",
    "#         weights[i] = fWeight \n",
    "#     weights = torch.tensor(weights)\n",
    "#     weights = weights.to(device)\n",
    "    \n",
    "#     crit = nn.BCELoss()\n",
    "\n",
    "#     return (output.double()-target.double()+1e-6)**2.00*(weights.double()).mean().float() \n",
    "# #     return crit(output, target)*weights.mean().float()\n",
    "# #     return sum((output.double()-target.double()).abs()).mean().float()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "        OLD ...\n",
    "        \n",
    "        #### MOTIF NET ####\n",
    "        self.conv1 = nn.Conv1d(22, 64, 5) ## 704 neurons\n",
    "        self.conv2 = nn.Conv1d(64, 128, 3) ## 384 neurons\n",
    "        self.conv3 = nn.Conv1d(128, 256, 3) ## 256 neurons\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        self.penult = nn.Linear(256, 128) \n",
    "        \n",
    "        #### COORD NET ####\n",
    "        self.mlp1 = nn.Linear(100, 704) \n",
    "        self.mlp2 = nn.Linear(704, 384)\n",
    "        self.mlp3 = nn.Linear(384, 256)\n",
    "        self.penult = nn.Linear(256, 128) \n",
    "        \n",
    "        ### CAT LAYERS ###\n",
    "        self.penult = nn.Linear(256, 128) \n",
    "        self.out = nn.Linear(128, len(fams))\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        A ...\n",
    "        \n",
    "        #### MOTIF NET ####\n",
    "        self.conv1 = nn.Conv1d(22, 16, 5) \n",
    "        self.cv_bn1 = nn.BatchNorm1d(16)\n",
    "        self.conv2 = nn.Conv1d(16, 32, 3) \n",
    "        self.cv_bn2 = nn.BatchNorm1d(32)\n",
    "        self.conv3 = nn.Conv1d(32, 64, 3) \n",
    "        self.cv_bn3 = nn.BatchNorm1d(64)\n",
    "        \n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        self.penult = nn.Linear(64, 32) \n",
    "        \n",
    "        #### COORD NET ####\n",
    "        self.mlp1 = nn.Linear(100, 80) \n",
    "        self.mlp2 = nn.Linear(80, 96)\n",
    "        self.mlp3 = nn.Linear(96, 64)\n",
    "        self.penult = nn.Linear(64, 32) \n",
    "        \n",
    "        ### CAT LAYERS ###\n",
    "        self.penult = nn.Linear(64, 32) \n",
    "        self.out = nn.Linear(32, len(fams))\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #############################################\n",
    "# # Get fam distance matrix for Phylo MSE loss.\n",
    "# #############################################\n",
    "\n",
    "# # # # # # # # #\n",
    "# my_embed = '14' \n",
    "# # # # # # # # #\n",
    "\n",
    "# pamFams = (np.genfromtxt('data_dev/fam_distances/fams.csv',dtype='U'))\n",
    "# pamMatrix = (np.genfromtxt('data_dev/fam_distances/distMatrix.csv',delimiter=',',dtype=float))\n",
    "\n",
    "# for i,each in enumerate(pamFams):\n",
    "#     pamFams[i] = each.upper()\n",
    "\n",
    "# famDistMatrix = np.full((len(fams),len(fams)),-1.0)\n",
    "# for fam in fams:\n",
    "#     fIdx1_set = np.where(fams==fam)[0][0]\n",
    "#     fIdx1_get = np.where(pamFams==fam)[0][0]\n",
    "#     for fam2 in fams:\n",
    "#         fIdx2_set = np.where(fams==fam2)[0][0]\n",
    "#         fIdx2_get = np.where(pamFams==fam2)[0][0]\n",
    "#         famDistMatrix[fIdx1_set][fIdx2_set] = pamMatrix[fIdx1_get][fIdx2_get]\n",
    "        \n",
    "# # normalize fam distances\n",
    "# fMax = np.max(famDistMatrix)\n",
    "# fMin = np.min(famDistMatrix)\n",
    "# famDistMatrix_scaled = np.array((famDistMatrix))\n",
    "# for i in range(len(fams)):\n",
    "#     for j in range(len(fams)):\n",
    "#         famDistMatrix_scaled[i][j] = float(famDistMatrix[i][j]-fMin)/(fMax-fMin) \n",
    "# famDistMatrix = famDistMatrix_scaled"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
